{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import folium\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display, IFrame\n",
    "from scripts.hist_module import *  #change back\n",
    "import requests\n",
    "import time\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Importing Data, Using Tables and Creating Graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Jupyter Notebook\n",
    "\n",
    "First of all, note that this page is divided into what are called \"cells\". For example, the following cell is a \"code cell\" where you will write your code. You'll see a `In [ ]:` next to each cell for code, which is a counter for the cells you have run. You can navigate cells by clicking on them or by using the up and down arrows. Cells will be highlighted as you navigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing cells\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "You can execute cells with <b><code>Ctrl-Enter</code></b> (which will run the cell and keep the same cell selected), or <b><code>Shift-Enter</code></b> (which will run the cell and then select the next cell).\n",
    "</div>\n",
    "\n",
    "Try running the following cell and see what it prints out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables\n",
    "\n",
    "### From Scratch\n",
    "\n",
    "If we don't have a spreadsheet file and are starting with nothing, first we need to make arrays. Arrays are simply a form of list that a programming language uses to denote a collection of items. In the case of a table, we'll consider an array as either a row or a column. Let's make two arrays below that will become our columns, one for famous psychologists and one for the year they were born. \n",
    "\n",
    "In general, to make an array we use: \n",
    "\n",
    "```python\n",
    "make_array(attribute_1, attribute_2, ...)\n",
    "```\n",
    "\n",
    "We set each of these created arrays equal to a variable name. This means that from now on, we can use that variable name to reference its respective array! Variables make information storage and retrieval much easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "psychologist_names = make_array(\"Freud\", \"Skinner\", \"Piaget\", \"Maslow\")\n",
    "psychologist_birth = make_array(1856, 1904, 1896, 1908)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've assigned these to variables, all we have to do is call the variable name to get the information back, or to manipulate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psychologist_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to make a table using these arrays, we use the general form:\n",
    "\n",
    "```python\n",
    "Table( ).with_columns(\"Column Name\", array_name, . . .)\n",
    "```\n",
    "\n",
    "We assign the created table to a variable (just like the arrays from above), and then type that variable name to display the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "psych_table= Table().with_columns(\"Psychologist\", psychologist_names,\n",
    "                                  \"Birth Year\", psychologist_birth)\n",
    "psych_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing\n",
    "\n",
    "It's more likey that a file holding your data already exists. In general, to import data from a file, we write:\n",
    "\n",
    "```python\n",
    "Table.read_table(\"file_name\")\n",
    "```\n",
    "\n",
    "Most often, these file names end in `.csv` to show the data format. `.csv` format is popular for spreadsheets and can be imported/exported from programs such as Microsoft Excel, OpenOffice Calc, or Google spreadsheets.\n",
    "\n",
    "We've scraped some data on [job postings](http://careers.historians.org/jobs/?page=1) for historians from the American Historical Association:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_data = Table.read_table('data/AHA-jobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's `show` the first 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python can calculate how large this table is with two functions: `num_rows` and `num_columns`. The general form for these functions are `table.num_rows` and `table.num_columns`. \n",
    "\n",
    "Let's use these on the table above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.num_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have 144 job postings, and 14 different columns of metadata for each posting.\n",
    "\n",
    "There are two methods to subset a table with select columns. We could either use the 'select' function or the 'drop' function. \n",
    "\n",
    "- `select` can create a new table with only the columns indicated in the parameters \n",
    "- `drop` can create a new table with columns NOT indicated in the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.select('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.select('title', 'employer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to select only a specific subset of rows, we can use the `where` method. The general form of this function is:\n",
    "\n",
    "```python\n",
    "table_name.where(column_name, predicate)\n",
    "```\n",
    "\n",
    "Let's look only at job postings `where` the `primary_field` is `equal_to` \"United States/North America\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.where(\"primary_field\", are.equal_to('United States/North America'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Tables Essentials!\n",
    "\n",
    "For your reference, here's a table of useful `Table` functions:\n",
    "\n",
    "|Name|Example|Purpose|\n",
    "|-|-|-|\n",
    "|`Table`|`Table()`|Create an empty table, usually to extend with data|\n",
    "|`Table.read_table`|`Table.read_table(\"my_data.csv\")`|Create a table from a data file|\n",
    "|`with_columns`|`tbl = Table().with_columns(\"N\", np.arange(5), \"2*N\", np.arange(0, 10, 2))`|Create a copy of a table with more columns|\n",
    "|`column`|`tbl.column(\"N\")`|Create an array containing the elements of a column|\n",
    "|`sort`|`tbl.sort(\"N\")`|Create a copy of a table sorted by the values in a column|\n",
    "|`where`|`tbl.where(\"N\", are.above(2))`|Create a copy of a table with only the rows that match some *predicate*|\n",
    "|`num_rows`|`tbl.num_rows`|Compute the number of rows in a table|\n",
    "|`num_columns`|`tbl.num_columns`|Compute the number of columns in a table|\n",
    "|`select`|`tbl.select(\"N\")`|Create a copy of a table with only some of the columns|\n",
    "|`drop`|`tbl.drop(\"2*N\")`|Create a copy of a table without some of the columns|\n",
    "|`take`|`tbl.take(np.arange(0, 6, 2))`|Create a copy of the table with only the rows whose indices are in the given array|\n",
    "|`join`|`tbl1.join(\"shared_column_name\", tbl2)`|Join together two tables with a common column name\n",
    "|`are.equal_to()`|`tbl.where(\"SEX\", are.equal_to(0))`|find values equal to that indicated|\n",
    "|`are.not_equal_to()`|`tbl.where(\"SEX\", are.not_equal_to(0))` | find values not including the one indicated|\n",
    "|`are.above()`| `tbl.where(\"AGE\", are.above(30))` | find values greater to that indicated|\n",
    "|`are.below()`| `tbl.where(\"AGE\", are.below(40))` | find values less than that indicated |\n",
    "|`are.between()`| `tbl.where(\"SEX\", are.between(18, 60))` | find values between the two indicated |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations \n",
    "\n",
    "Now that we have a manageable table we can start making visualizations! We're going to make our data easier to work with by using the `group()` function. This function will group all the data from a specific column by the amount of times it shows up throughout the table \n",
    "\n",
    "An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.group('primary_field')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's organize the count table so that it puts the entries with the highest counts at the top of the table. We will do this by adding the `descending=True` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.group('primary_field').sort('count', descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to make our first visualization. We are going to create a bar graph of the data in the above cell, since it is grouped by counts. \n",
    "\n",
    "To create a bar graph, we need to use either the `bar()` or `barh()` function. The added `h` in `barh()` makes the graph appear horizontally instead of vertically. The general form is:\n",
    "\n",
    "```python\n",
    "table.barh(\"column to graph\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.group('primary_field').barh('primary_field')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's create some more bar graphs for other columns in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.group('type').barh('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.group('preferred_education').barh('preferred_education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Analysis \n",
    "\n",
    "Let's use some of the results from above to perform an exploratory data analysis of our job listing data. \n",
    "\n",
    "It is important to note that performing quantitative data analysis on data sets such as the job listings set that we are using is quite difficult. This is due to the fact that the data doesn't have any continuous numerical variables that we can really use to see trends and correlations. In other words, most of our data is general qualitative descriptions such as job descriptions or specific job titles. The `salary` column is a potential place of quantitative analysis interest, but unfortunately most jobs list the salary as 'Open', making the analysis less meaningful.\n",
    "\n",
    "However, we can still try to analyze our data quantitatively. We just need to play around with it. \n",
    "\n",
    "Specifically, let's see if there is a correlation between job type (Experienced, Entry Level, Internship) and preferred education level (Doctorate, Master, None). To do this, we will assign numerical values to represent each category. Higher numerical values will correspond to higher positions/educations.\n",
    "\n",
    "The cell below goes through each row in the table, and stores its `preferred_education` and `type` information in two seperate lists. Think of the `education_points` list as a list of x-coordinates and the `type_points` list as a list of y-coordinates. \n",
    "\n",
    "Our goal is to use those lists to eventually plot the data and look for a correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_points = []\n",
    "type_points = []\n",
    "for index in range(0, job_data.num_rows):\n",
    "    curr_row = job_data.row(index)\n",
    "    if curr_row[9] == '- Internship':\n",
    "        type_points.append(1)\n",
    "    elif curr_row[9] == '- Entry Level':\n",
    "        type_points.append(2)\n",
    "    elif curr_row[9] == '- Experienced':\n",
    "        type_points.append(3)\n",
    "        \n",
    "    if curr_row[7] == 'nan':\n",
    "        education_points.append(1)\n",
    "    elif curr_row[7] == 'Masters':\n",
    "        education_points.append(2)\n",
    "    elif curr_row[7] == 'Doctorate':\n",
    "        education_points.append(3)\n",
    "        \n",
    "edu_np = np.array(education_points)\n",
    "type_np = np.array(type_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a table of those two lists and see what we can do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_type = Table().with_columns('Education', edu_np, 'Type', type_np)\n",
    "education_type.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to find the 'correlation coefficient' of this data. \n",
    "\n",
    "---\n",
    "\n",
    "### The correlation coefficient - *r*\n",
    "\n",
    "> The correlation coefficient ranges from −1 to 1. A value of 1 implies that a linear equation describes the relationship between X and Y perfectly, with all data points lying on a line for which Y increases as X increases. A value of −1 implies that all data points lie on a line for which Y decreases as X increases. A value of 0 implies that there is no linear correlation between the variables. ~Wikipedia\n",
    "\n",
    "---\n",
    "\n",
    "*r* = 1: the scatter diagram is a perfect straight line sloping upwards\n",
    "\n",
    "*r* = -1: the scatter diagram is a perfect straight line sloping downwards.\n",
    "\n",
    "Let's calculate the correlation coefficient between preferred education and job type. We can use the `np.corrcoef` function on the two variable (columns here) that we want to correlate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(education_type['Education'], education_type['Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a matrix for each variable. We have two 1s because, of course, each variable is perfectly correlated with itself. Our coefficient here is around -0.17, which indicates almost no correlation. That tells us that at least for this data set, jobs with higher positions don't neccessarily necessitate the highest degrees of education. \n",
    "\n",
    "We can also create a quick scatter plot of our table data. Note that the scatter plot for this data isn't actually too useful, since many values in the table are repeated. \n",
    "\n",
    "To create a bar graph, we need to use either the `scatter()` function. The general form is:\n",
    "\n",
    "```python\n",
    "table.scatter(\"column for x-axis\", \"column for y-axis\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_type.scatter('Education', 'Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, the scatter plot doesn't really provide us with too much information. When performing data analysis, it is always important to try a lot of different analysis/visualization techniques, since each technique has its pros and cons and excels in different situation. In this case, our correlation coefficient analysis gave us more information than the scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text Analysis\n",
    "\n",
    "We can also perform data analysis with the textual information in our dataset. Based on the non-numerical style of our dataset, we may be able to make more meaningful conclusions about our data. \n",
    "\n",
    "We are going to be using the `spaCy` library for our text analysis. The package contains a lot of powerful functions for large-scale text analysis and natural language processing.\n",
    "\n",
    "The first thing we are going to do is load in English as our language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the textual information in our table. Below, we look at the first entry in the `job_description` column in our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data['job_description'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use spaCy's `nlp` function to analyze the paragraph from the cell above. The `nlp` function is very powerful, and has the ability to break up a text into sentences/phrases. `parsed_text` will be the variable that contains our information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_text = nlp(str(job_data['job_description'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print out the length of the parsed text as well as the number of sentences that it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(parsed_text.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the `parsed_text` variable contains information about each of the sentences in our original paragraph. We are going to take that information and turn it into a table to perform further analysis. The code in the cell below will go through every sentence that the `parsed_text` variable contains and put each one in a new row in our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_tab = Table()\n",
    "sents_tab.append_column(label=\"Sentence\", values=[sentence.text for sentence in parsed_text.sents])\n",
    "sents_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do the same thing once again, however with words this time instead of sentences. The `parsed_text` variable also contains information about the individual words in our original paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toks_tab = Table()\n",
    "toks_tab.append_column(label=\"Word\", values=[word.text for word in parsed_text])\n",
    "toks_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another column to our table of words. The `parsed_text` variable from the `nlp` function earlier also contains information about each word's part of speech. We are going to append that information to our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_tab.append_column(label=\"POS\", values=[word.pos_ for word in parsed_text])\n",
    "toks_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to add one more column to the table from the above cell. This column is going to contain the 'Lemma' of each word. The \"Lemma\" of a word is that word's 'base form'. Once again, we will query the `parsed_text` variable for this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toks_tab.append_column(label=\"Lemma\", values=[word.lemma_ for word in parsed_text])\n",
    "toks_tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's great that we have all this data in individual tables. However, it would be most useful to have it all in one table. We define a function below that takes in a variable like `parsed_text` and outputs one table that contains all the information from the last few cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tablefy(parsed_text):\n",
    "    toks_tab = Table()\n",
    "    toks_tab.append_column(label=\"Word\", values=[word.text for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"POS\", values=[word.pos_ for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Lemma\", values=[word.lemma_ for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Stop Word\", values=[word.is_stop for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Punctuation\", values=[word.is_punct for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Space\", values=[word.is_space for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Number\", values=[word.like_num for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"OOV\", values=[word.is_oov for word in parsed_text])\n",
    "    toks_tab.append_column(label=\"Dependency\", values=[word.dep_ for word in parsed_text])\n",
    "    return toks_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our `tablefy()` function on our original `parsed_text` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablefy(parsed_text).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's really cool! All of our extremely detailed information about that one entry is now in one place. \n",
    "\n",
    "Now, let's get the same type of information for all the entries in the `job_description` column. Recall that the table above only contains the information about the first entry. \n",
    "\n",
    "In order to get information from the whole column, we are going to pass it in entirely to the `nlp` function. If you look at the output, you'll see that over 60,000 rows are omitted (from being displayed). That's a lot of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptions = tablefy(nlp(' '.join(job_data['job_description'])))\n",
    "all_descriptions.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform more analysis using some of the useful `Table()` functions from the previous sections. Let's first subset our table by only selecting the rows where the `POS` column is equal to `ADJ`. In other words, we will only keep the words that are adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adjectives = all_descriptions.where('POS', are.equal_to('ADJ'))\n",
    "adjectives.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our smaller table, let us take counts of how many times each word appears, and display that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(adjectives['Word']).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some of the most common words in our display above don't really provide us with too much information. For example, the words 'their' and 'that' appear very often in our data, but don't really provide us with insight into the data since they are simply just common words that are used in the English language. It would be ideal if we removed them from our data in order to only keep the words that are actually useful and significant.\n",
    "\n",
    "Fortunately, spaCy has functionality to do this. In the `Stop Word` column, each word either has a True or a False depending on whether it is one of the most common words in the language (part of a stop list). We are going to subset our data again by picking out the adjectives. However this time, we are only going to keep the adjectives that are not stop words (i.e that are not very common English words like 'their' and 'that')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = all_descriptions.where('POS', are.equal_to('ADJ')).where('Stop Word', are.equal_to(False))\n",
    "Counter(adjectives['Word']).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for words within the 'count' information. If I am interested in the amount of times that the word 'digital' shows up in the table, I can query the Counter for that word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(adjectives['Word'])['digital']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try doing the same analysis, but with nouns instead of adjectives. Once again, we will only be keeping words that are not 'stop words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives = all_descriptions.where('POS', are.equal_to('NOUN')).where('Stop Word', are.equal_to(False))\n",
    "Counter(adjectives['Word']).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "Verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latitude = []\n",
    "longitude = []\n",
    "for i in range(job_data.num_rows):\n",
    "    search = urllib.parse.quote(job_data['employer'][i])\n",
    "    \n",
    "    print(job_data['employer'][i])\n",
    "\n",
    "    try:\n",
    "        json_res = requests.get('https://maps.googleapis.com/maps/api/geocode/json?address={}'.format(search)).json()\n",
    "        coordinates = json_res['results'][0]['geometry']['location']\n",
    "        latitude.append(coordinates['lat'])\n",
    "        longitude.append(coordinates['lng'])\n",
    "    except:\n",
    "        latitude.append('')\n",
    "        longitude.append('')\n",
    "\n",
    "    time.sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job_data = job_data.with_columns('latitude', latitude, 'longitude', longitude)\n",
    "job_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_dict, html_key = assign_colors(job_data.to_df(), \"employment_type\")\n",
    "display(HTML(html_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapa = folium.Map(location=[39.8333333,-98.585522], zoom_start=3) # Folium is a useful library for generating\n",
    "                                                                   # Google maps-like map visualizations.\n",
    "for r in job_data.rows:\n",
    "    \n",
    "    if r[-2] != '':\n",
    "        folium.CircleMarker((float(r[-2]), float(r[-1])),\n",
    "                    radius=1,\n",
    "                    popup=r[10],\n",
    "                    color=color_dict[r[10]],\n",
    "                    fill_color=color_dict[r[10]],\n",
    "                   ).add_to(mapa)\n",
    "\n",
    "mapa.save(\"map1.html\")\n",
    "IFrame('map1.html', width=700, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
